{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for token classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and extract the two columns we need from the conll dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['words', 'labels'],\n",
      "    num_rows: 14041\n",
      "})\n",
      "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'] [3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "print(raw_datasets)\n",
    "raw_datasets = raw_datasets.remove_columns([\"chunk_tags\", \"id\", \"pos_tags\"])\n",
    "raw_datasets = raw_datasets.rename_columns({\"tokens\": \"words\", \"ner_tags\":\"labels\"})\n",
    "print(raw_datasets[\"train\"])\n",
    "print(raw_datasets[\"train\"][0][\"words\"], raw_datasets[\"train\"][0][\"labels\"])\n",
    "\n",
    "label_names = raw_datasets[\"train\"].features[\"labels\"].feature.names\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B-XXX labels are used at the beginning of an entity while I-XXX labels are used for the following words\n",
    "#### To tokenize text already split into words, we just need to set ***is_split_into_words=True***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    " \n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"words\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once tokenized, we get more inputs than labels, so we need to do the alignment\n",
    "<img src=\"image/data_preprocessing2.png\" width=800>\n",
    "\n",
    "#### Tokenizer use the word ID to match each token with the label of its word\n",
    "<img src=\"image/data_preprocessing1.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "}) \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def shift_label(label):\n",
    "    if label % 2 == 1:\n",
    "        label += 1\n",
    "    return label\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word=None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != current_word:\n",
    "            current_word = word_id\n",
    "            new_labels.append(labels[word_id])\n",
    "        else:\n",
    "            new_labels.append(shift_label(labels[word_id]))\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokneized_inputs = tokenizer(examples[\"words\"], truncation=True, is_split_into_words=True)\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokneized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokneized_inputs[\"labels\"] = new_labels\n",
    "    return tokneized_inputs\n",
    "\n",
    "tokenized_datasets =raw_datasets.map(tokenize_and_align_labels, batched=True)\n",
    "print(raw_datasets, \"\\n\", tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last step is batching the inputs together, the input need to be padded to the same length\n",
    "#### It can be done by a data collator designed for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for masked language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset should be set in a format with just one column of texts, we will need to batch them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first way to group sentences is just to pad or truncate the texts to the length we set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 25000/25000 [00:02<00:00, 9848.36 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")\n",
    "raw_datasets = raw_datasets.remove_columns(\"label\")\n",
    "\n",
    "model_ckpt = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "context_length = 128\n",
    "\n",
    "def tokenzie_pad_and_truncate(text):\n",
    "    return tokenizer(text[\"text\"], truncation=True, padding=\"max_length\", max_length=context_length)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenzie_pad_and_truncate, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, if the text is very long it will lose a lot in trucation\n",
    "#### Instead, we could generate several blocks of contexts length from that very long text, it can be done by asking the tokenizer to return overflowing tokens by setting ***return_overflowing_tokens=True***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5444.33 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5764.58 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:08<00:00, 5813.22 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 76257)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_chunk(texts):\n",
    "    return tokenizer(\n",
    "        texts[\"text\"], truncation=True, max_length=context_length, return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_chunk, batched=True, remove_columns=\"text\"\n",
    ")\n",
    "\n",
    "len(raw_datasets[\"train\"]), len(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your texts have various lengths, one option is to concatenate them all, then taking chunks of context length\n",
    "<img src=\"image/data_preprocessing4.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:02<00:00, 9945.00 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 63257)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_chunk(texts):\n",
    "    all_input_ids=[]\n",
    "    for input_ids in tokenizer(texts[\"text\"])[\"input_ids\"]:\n",
    "        all_input_ids.extend(input_ids)\n",
    "        all_input_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    chunks = []\n",
    "    for idx in range(0, len(all_input_ids), context_length):\n",
    "        chunks.append(all_input_ids[idx:idx+context_length])\n",
    "    return {\"input_ids\": chunks}\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_and_chunk, batched=True, remove_columns=[\"text\"])\n",
    "len(raw_datasets[\"train\"]), len(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The masking in itself is done by the data collator designed for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and extract the two columns we need from teh kde4 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'translation'],\n",
      "    num_rows: 210173\n",
      "})\n",
      "Dataset({\n",
      "    features: ['inputs', 'targets'],\n",
      "    num_rows: 210173\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\", trust_remote_code=True, split=\"train\")\n",
    "print(raw_datasets)\n",
    "\n",
    "def extract_language(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    return {\"inputs\": inputs, \"targets\": targets}\n",
    "\n",
    "raw_datasets = raw_datasets.map(extract_language, batched=True, remove_columns=[\"id\", \"translation\"])\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to tokenize both the inputs and the targets, as the inputs and target use English and Franch tokenizer, we need to warn the tokenizer use the different tokenizer using the ***with tokenizer.as_target_tokenizer:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Using', '▁the', '▁B', 'abel', 'fish', '▁plugin', '</s>'] ['▁Utilisation', '▁du', '▁module', '▁externe', '▁B', 'abel', 'f', 'ish', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "sample = raw_datasets[12]\n",
    "inputs = tokenizer(sample[\"inputs\"])\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(sample[\"targets\"])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]), tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 210173\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(example):\n",
    "    model_inputs = tokenizer(example[\"inputs\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"targets\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"inputs\", \"targets\"]\n",
    ")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the maximum length of the input and target may be different, we need to pad the input and target separately\n",
    "#### The input is padded with pad token, and the target with -100 index to make sure are not taken into account in the loss computation\n",
    "<img src=\"image/data_preprocessing6.png\" width=800>\n",
    "\n",
    "#### It can be inplemented by using the ***DataCollatorForSeq2Seq***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataset for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and extract the two columns we need form the xsum dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 204045\n",
      "}) \n",
      " {'document': 'A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\\nAs they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames.\\nOne of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\\nPolice have appealed for information about the attack.\\nInsp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.\\n\"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"', 'summary': 'Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\", split=\"train\")\n",
    "raw_datasets = raw_datasets.remove_columns([\"id\"])\n",
    "print(raw_datasets, \"\\n\", raw_datasets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize both inputs and targets\n",
    "#### As the special tokens we add might be different for the inputs and the targets, the ***with tokenizer.as_target_tokenizer():*** need to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁A', '▁fire', '▁alarm', '▁went', '▁off', '▁at', '▁the', '▁Holiday', '▁In', 'n', '▁in', '▁Hope', '▁Street', '▁at', '▁about', '▁04', ':', '20', '▁B', 'ST', '▁on', '▁Saturday', '▁and', '▁guests', '▁were', '▁asked', '▁to', '▁leave', '▁the', '▁hotel', '.', '▁As', '▁they', '▁', 'gathered', '▁outside', '▁they', '▁saw', '▁the', '▁two', '▁buses', ',', '▁', 'parked', '▁side', '-', 'by', '-', 'side', '▁in', '▁the', '▁car', '▁park', ',', '▁', 'en', 'gul', 'fed', '▁by', '▁flame', 's', '.', '▁One', '▁of', '▁the', '▁tour', '▁groups', '▁is', '▁from', '▁Germany', ',', '▁the', '▁other', '▁from', '▁China', '▁and', '▁Taiwan', '.', '▁It', '▁was', '▁their', '▁first', '▁night', '▁in', '▁Northern', '▁Ireland', '.', '▁The', '▁driver', '▁of', '▁one', '▁of', '▁the', '▁buses', '▁said', '▁many', '▁of', '▁the', '▁passengers', '▁had', '▁left', '▁personal', '▁belonging', 's', '▁on', '▁board', '▁and', '▁these', '▁had', '▁been', '▁destroyed', '.', '▁Both', '▁groups', '▁have', '▁organised', '▁replacement', '▁coaches', '▁and', '▁will', '▁begin', '▁their', '▁tour', '▁of', '▁the', '▁north', '▁coast', '▁later', '▁than', '▁they', '▁had', '▁planned', '.', '▁Police', '▁have', '▁appeal', 'e', 'd', '▁for', '▁information', '▁about', '▁the', '▁attack', '.', '▁In', 's', 'p', '▁David', '▁Gibson', '▁said', ':', '▁\"', 'I', 't', '▁appears', '▁as', '▁though', '▁the', '▁fire', '▁started', '▁under', '▁one', '▁of', '▁the', '▁buses', '▁before', '▁spreading', '▁to', '▁the', '▁second', '.', '▁\"', 'While', '▁the', '▁exact', '▁cause', '▁is', '▁still', '▁under', '▁investigation', ',', '▁it', '▁is', '▁thought', '▁that', '▁the', '▁fire', '▁was', '▁started', '▁deliberately', '.\"', '</s>'] \n",
      " ['▁Two', '▁tourist', '▁buses', '▁have', '▁been', '▁destroyed', '▁by', '▁fire', '▁in', '▁', 'a', '▁suspected', '▁ar', 'son', '▁attack', '▁in', '▁Belfast', '▁city', '▁centre', '.', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "sample = raw_datasets[1]\n",
    "inputs = tokenizer(sample[\"document\"])\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(sample[\"summary\"])\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]), \"\\n\", tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/204045 [00:00<?, ? examples/s]/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 204045/204045 [00:54<00:00, 3775.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 204045\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_input = tokenizer(examples[\"document\"], max_length=max_input_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], truncation=True, max_length=max_target_length)\n",
    "\n",
    "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_input\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function, batched=True, remove_columns=[\"document\", \"summary\"]\n",
    ")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We pad the inputs and targets separately, as their maximum length are completely different\n",
    "#### The input is padded with pad token and the target is padded with -100 index to make sure they are not taken into sccount in the loss computation\n",
    "<img src=\"image/data_preprocessing5.png\" width=800>\n",
    "\n",
    "#### It can be accomplished by using the ***DataCollatorForSeq2Seq***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for casual language modeling\n",
    "#### The inputs also serve as the label in casual language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to chunk the input sequence into context size pieces\n",
    "<img src=\"image/data_preprocessing7.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=raw_datasets.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When dealing with long context or short sequences we should concatenate first, as the short sequence will be discard due to shorter thant the context length\n",
    "#### First tokenize each sample without truncation, then concatenate tokenized sample with and EOS token in between\n",
    "<img src=\"image/data_preprocessing8.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aligning labels is handled inside the model and we can just pass the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "loss = output.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and extract the four columns we need from the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'question', 'answers'],\n",
      "    num_rows: 87599\n",
      "}) \n",
      " {'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n",
      "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
      "Dataset({\n",
      "    features: ['context', 'question', 'answer_start', 'answer_end'],\n",
      "    num_rows: 87599\n",
      "})\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\", split=\"train\")\n",
    "raw_datasets = raw_datasets.remove_columns([\"id\", \"title\"])\n",
    "print(raw_datasets, \"\\n\", raw_datasets.features)\n",
    "print(raw_datasets[0][\"answers\"])\n",
    "\n",
    "def prepare_data(example):\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "    example[\"answer_start\"] = example[\"answers\"][\"answer_start\"][0]\n",
    "    example[\"answer_end\"] = example[\"answer_start\"] + len(answer)\n",
    "    return example\n",
    "\n",
    "raw_datasets = raw_datasets.map(prepare_data, remove_columns=[\"answers\"])\n",
    "print(raw_datasets)\n",
    "print(f\"Context: {raw_datasets[0]['context']}\\nQuestion: {raw_datasets[0]['question']}\\nAnswer: {raw_datasets[0]['context'][raw_datasets[0]['answer_start']:raw_datasets[0]['answer_end']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When the context is very long it may be truncated by the tokenizer, instead, we create several features for different pieces of the context\n",
    "#### But to avoid truncating the answer, we allow some overlap\n",
    "<img src=\"image/data_preprocessing10.png\" width=800>\n",
    "\n",
    "#### This is done automatically by the tokenizer if use ***return_overflowing_tokens=True***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "example = raw_datasets[0]\n",
    "inputs = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    truncation='only_second',\n",
    "    padding=\"max_length\",\n",
    "    max_length=384,\n",
    "    stride=128,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "print(len(inputs[\"offset_mapping\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we have tokenized our inputs, we assign labels like this\n",
    "<img src=\"image/data_preprocessing11.png\" width=800>\n",
    "\n",
    "#### When context does not contain the answer, we set the labels to the CLS token index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]]\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "print(inputs.sequence_ids(0))\n",
    "print(inputs[\"offset_mapping\"])\n",
    "print(inputs[\"overflow_to_sample_mapping\"])\n",
    "\n",
    "\n",
    "def find_labels(offsets, answer_start, answer_end, sequence_ids):\n",
    "    idx = 0\n",
    "    # find the context position base on the sequence id, question label 0, context label 1, padding label None\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] ==1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # if the answer is not fully in the context, return (0, 0)\n",
    "    if offsets[context_start][0] > answer_end or offsets[context_end][1] < answer_start:\n",
    "        return (0, 0)\n",
    "    else:\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offsets[idx][0] <= answer_start:\n",
    "            idx += 1\n",
    "        start_position = idx -1\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offsets[idx][0] >= answer_end:\n",
    "            idx -= 1\n",
    "        end_position = idx + 1\n",
    "        return start_position, end_position\n",
    "    \n",
    "start, end = find_labels(\n",
    "    # the offset_mapping dim is 2 due to the overflow issue\n",
    "    inputs[\"offset_mapping\"][0],\n",
    "    example[\"answer_start\"],\n",
    "    example[\"answer_end\"],\n",
    "    inputs.sequence_ids(0)\n",
    ")\n",
    "tokenizer.decode(inputs[\"input_ids\"][0][start: end])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 88729\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    offset_mapping =inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    inputs[\"start_positions\"] = []\n",
    "    inputs[\"end_positions\"] = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        start, end = find_labels(\n",
    "            offset,\n",
    "            examples[\"answer_start\"][sample_idx],\n",
    "            examples[\"answer_end\"][sample_idx],\n",
    "            inputs.sequence_ids(i)\n",
    "        )\n",
    "\n",
    "        inputs[\"start_positions\"].append(start)\n",
    "        inputs[\"end_positions\"].append(end)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_training_examples,\n",
    "                                      batched=True,\n",
    "                                      remove_columns=raw_datasets.column_names)\n",
    "print(tokenized_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
