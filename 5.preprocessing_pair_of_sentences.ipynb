{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The text classification can be applied on pair of sentences\n",
    "<div>\n",
    "<img src=\"image/pair_of_sentence1.png\" width=800/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### In GLUE benckmark, 8 of 10 tasks concern pairs of sentences\n",
    "<div>\n",
    "<img src=\"image/pair_of_sentence2.png\" width=800/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model like BERT are pretrained to recognize relationships between two sentences. For training, BERT will show pairs of sentences and need to predict both the value of the randomly masked tokens and whether the second sentence follows the first sentence\n",
    "<div><img src=\"image/pair_of_sentence3.png\" width=800></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***AutoTokenizer*** instance accept sentence pairs as well as single sentence, the ***token_type_ids*** can be indexed from the ***tokenizer*** output to get a mask indicating which sentence the token belongs to\n",
    "<div><img src=\"image/pair_of_sentence4.png\" width=1000></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2026, 2171, 2003, 18816, 3148, 2080, 15990, 2063, 102, 1045, 2572, 1037, 2822, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "output = tokenizer(\"My name is Qiyao Xue\", \"I am a Chinese\")\n",
    "print(output)\n",
    "print(output[\"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To process several pairs of sentences together, just pass the list of first sentences followed by the list of second sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: {'input_ids': [[101, 2026, 2171, 2003, 18816, 3148, 2080, 15990, 2063, 1012, 102, 2023, 2003, 1037, 17662, 12172, 2607, 1012, 102], [101, 1045, 2572, 1037, 2822, 1012, 102, 2009, 2003, 2307, 1012, 102, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "input_ids: [[101, 2026, 2171, 2003, 18816, 3148, 2080, 15990, 2063, 1012, 102, 2023, 2003, 1037, 17662, 12172, 2607, 1012, 102], [101, 1045, 2572, 1037, 2822, 1012, 102, 2009, 2003, 2307, 1012, 102, 0, 0, 0, 0, 0, 0, 0]]\n",
      "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xueqiyao/anaconda3/envs/Pytorch/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "output = tokenizer(\n",
    "    [\"My name is Qiyao Xue.\", \"I am a Chinese.\"],\n",
    "    [\"This is a HuggingFace course.\", \"It is great.\"],\n",
    "    padding=True\n",
    "    )\n",
    "print(f\"output:\", output)\n",
    "print(f\"input_ids:\", output[\"input_ids\"])\n",
    "print(f\"token_type_ids:\", output[\"token_type_ids\"])\n",
    "print(f\"attention_mask:\", output[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Those inputs are then ready to go through a sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2570, -0.7383],\n",
      "        [-0.1203, -0.4321]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "batch = tokenizer(\n",
    "    [\"My name is Qiyao Xue.\", \"I am a Chinese.\"],\n",
    "    [\"This is a HuggingFace course.\", \"It is great.\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**batch)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6181, 0.3819],\n",
      "        [0.5773, 0.4227]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "sentence pair 1: LABEL_0, sentence pair 2: LABEL_0\n"
     ]
    }
   ],
   "source": [
    "id2label = model.config.id2label\n",
    "print(id2label)\n",
    "print(f\"sentence pair 1: {id2label[predictions[0].argmax().item()]}, sentence pair 2: {id2label[predictions[1].argmax().item()]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
