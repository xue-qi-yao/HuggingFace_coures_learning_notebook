{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pipline function returns and end-to-end object that performs and NLP task on one or several texts, it supports most common NLP tasks out of the box.\n",
    "#### The pipline consists of three stages\n",
    "<div>\n",
    "<img src=\"image/pipeline1.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first task for trying the pipeline API on is ***sentiment analysis***, it classifies texts as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9433631896972656}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I have been waiting for a HuggingFace course my whole life.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple texts can be passed to the object returned by a pipeline to treat them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9433631896972656}, {'label': 'NEGATIVE', 'score': 0.9995473027229309}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier([\"I have been waiting for a HuggingFace course my whole life.\",\n",
    "                    \"I hate it so much!\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***zero-shot-classification*** pipeline lets you selecet the labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is a course about the Transfomers library.', 'labels': ['education', 'bussiness', 'politics'], 'scores': [0.8097766041755676, 0.14343445003032684, 0.04678897187113762]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "result = classifier(\n",
    "    \"This is a course about the Transfomers library.\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"bussiness\"]\n",
    "    )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***text-generation*** pipeline uses an input prompt to generate text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to build and install the most versatile BluetoothÂ® Smart Bluetooth devices. We will also use your mobile telephone to communicate with your music and music apps using your mobile phone. As you know, it is well known'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "result = generator(\"In this course, we will teach you how to\",\n",
    "                   pad_token_id=generator.tokenizer.eos_token_id\n",
    "                   )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each task, you can search the model hub for various models to use in the pipeline: [HuggingFace model hub](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is another ***text generation*** pipeline, using the ***distilgpt2*** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In this course, we will teach you how to apply to all topics on the subject in this class. As a result, you will gain experience through'}, {'generated_text': 'In this course, we will teach you how to manipulate the computer and use different algorithms and techniques. The lessons we will teach you to apply you to'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\",\n",
    "                     model=\"distilgpt2\"\n",
    "                     )\n",
    "result = generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***fill-mask*** pipeline  will predict missing words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision ec58a5b (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.19198395311832428, 'token': 30412, 'token_str': ' mathematical', 'sequence': 'This course will teach you all about mathematical models.'}, {'score': 0.04209199175238609, 'token': 38163, 'token_str': ' computational', 'sequence': 'This course will teach you all about computational models.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "result = unmasker(\n",
    "    \"This course will teach you all about <mask> models.\",\n",
    "    top_k=2\n",
    "    )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***NER*** pipeline indentifies entities such as persons, organizations or locations in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9850975, 'word': 'Qiyao Xue', 'start': 11, 'end': 20}, {'entity_group': 'ORG', 'score': 0.9732388, 'word': 'HuggingFace', 'start': 43, 'end': 54}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "result = ner(\"My name is Qiyao Xue and I am studying the HuggingFace transformers library.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***question-answering*** pipeline extracts answers to a question from a given context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9962271451950073, 'start': 34, 'end': 44, 'answer': 'Pittsburgh'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "QA = pipeline(\"question-answering\")\n",
    "result = QA(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is QiyaoXue and I work at Pittsburgh\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***summarization*** pipeline creates summaries of long texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Your min_length=56 must be inferior than your max_length=18.\n",
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1282: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (18). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' Hugging Face, Inc. develops computation tools for building applications using machine learning'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "result = summarizer(\"\"\"Hugging Face, Inc. is an American company incorporated under the Delaware General Corporation Law[1] and based in New York City that develops computation tools for building applications using machine learning. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work.\"\"\",\n",
    "                    max_length=18\n",
    "                    )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***translation*** pipeline translate text from one language to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'This course is produced by HuggingFace.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\",\n",
    "                      model=\"Helsinki-NLP/opus-mt-fr-en\"\n",
    "                      )\n",
    "result = translator(\"Ce cours est produit par HuggingFace.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The pipline consists of three stages\n",
    "<div>\n",
    "<img src=\"image/pipeline1.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage1: Tokenlization\n",
    "<div>\n",
    "<img src=\"image/pipeline2.png\" width='800'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***AutoTokenizer*** class can load the tokeniszer for any checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  2031,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2009,  2061,  2172,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I have been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate it so much\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# padding=True: padding science the two sentences are not of the same size, the shortest one need to be padded to be able to build an array\n",
    "# truncation=True: ensure that anyt sentence longer than the maximum the model can handle is truncated\n",
    "# return_tensors=\"pt\": let tokenizer returns pytorch tensor\n",
    "# the output attention_mask indicate where the padding has been applied, so the model not pay attention to it\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage2: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ***AutoModel*** class loads a modle without its pretaining head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "# the output [2, 15, 768] refers to [batch size, sequence length, hidden size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each ***AutoModelForXXX*** class loads a model sutiable for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3782,  1.4346],\n",
      "        [ 4.3257, -3.4977]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)\n",
    "# logits refers to the last layer FFN output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage3: Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To go from logits to probabilities we apply a SoftMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.6637e-02, 9.4336e-01],\n",
      "        [9.9960e-01, 4.0010e-04]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n",
      "sentence1: POSITIVE, sentence2: NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "id2label = model.config.id2label\n",
    "print(id2label)\n",
    "print(f\"sentence1: {id2label[torch.argmax(predictions[0]).item()]}, sentence2: {id2label[torch.argmax(predictions[1]).item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the token classification pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The token classification pipeline gives each token in the sentence a label, whether each word corresponding to a person, an organization or a location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.99662614, 'index': 4, 'word': 'Qi', 'start': 11, 'end': 13}, {'entity': 'I-PER', 'score': 0.9735996, 'index': 5, 'word': '##ya', 'start': 13, 'end': 15}, {'entity': 'I-PER', 'score': 0.94115335, 'index': 6, 'word': '##o', 'start': 15, 'end': 16}, {'entity': 'I-PER', 'score': 0.9444561, 'index': 7, 'word': '##X', 'start': 16, 'end': 17}, {'entity': 'I-PER', 'score': 0.9376296, 'index': 8, 'word': '##ue', 'start': 17, 'end': 19}, {'entity': 'I-LOC', 'score': 0.99976164, 'index': 13, 'word': 'China', 'start': 34, 'end': 39}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "result = token_classifier(\"My name is QiyaoXue and I am from China\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It can also group together tokens corresponding to the same entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9586929, 'word': 'QiyaoXue', 'start': 11, 'end': 19}, {'entity_group': 'LOC', 'score': 0.99976164, 'word': 'China', 'start': 34, 'end': 39}]\n"
     ]
    }
   ],
   "source": [
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "agg_result = token_classifier(\"My name is QiyaoXue and I am from China\")\n",
    "print(agg_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "torch.Size([1, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Qiyao Xue and I am a Chinese.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id2label{0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n",
      "prediction id:[0, 0, 0, 0, 4, 4, 4, 4, 4, 0, 0, 0, 0, 2, 0, 0]\n",
      "prediction label:['O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'I-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
    "predictions = probabilities.argmax(dim=-1).tolist()\n",
    "print(f\"id2label{model.config.id2label}\")\n",
    "print(f\"prediction id:{predictions}\\nprediction label:{[model.config.id2label[predict] for predict in predictions]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The start and end character positions can be found settignt the ***return_offset_mapping=True*** when giving text input to the tokenizer, the returned start, end index is left close right open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1422, 1271, 1110, 24357, 2315, 1186, 17584, 1162, 1105, 146, 1821, 170, 1922, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 7), (8, 10), (11, 13), (13, 15), (15, 16), (17, 19), (19, 20), (21, 24), (25, 26), (27, 29), (30, 31), (32, 39), (39, 40), (0, 0)]}\n",
      "[{'entity': 'I-PER', 'score': 0.9963464140892029, 'word': 'Qi', 'start': 11, 'end': 13}, {'entity': 'I-PER', 'score': 0.9570150375366211, 'word': '##ya', 'start': 13, 'end': 15}, {'entity': 'I-PER', 'score': 0.9778035283088684, 'word': '##o', 'start': 15, 'end': 16}, {'entity': 'I-PER', 'score': 0.9940474033355713, 'word': 'Xu', 'start': 17, 'end': 19}, {'entity': 'I-PER', 'score': 0.9704844951629639, 'word': '##e', 'start': 19, 'end': 20}, {'entity': 'I-MISC', 'score': 0.9973861575126648, 'word': 'Chinese', 'start': 32, 'end': 39}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "input_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "print(input_with_offsets)\n",
    "tokens = input_with_offsets.tokens()\n",
    "offsets = input_with_offsets[\"offset_mapping\"]\n",
    "zero_label = model.config.id2label[0]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != zero_label:\n",
    "        start, end = offsets[idx]\n",
    "        results.append({\"entity\": label, \"score\": probabilities[idx][pred].item(), \"word\": tokens[idx], \"start\": start, \"end\": end})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are generally two way of labeling to do the token classification\n",
    "* use the B-XXX label at the beginning of each new entity\n",
    "* use the B-XXX label to separate two adjacent entities of the same type\n",
    "<div><img src=\"image/pipeline3.png\" width=800></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'word': 'Qiyao Xue', 'start': 11, 'end': 20}, {'entity_group': 'MISC', 'word': 'Chinese', 'start': 32, 'end': 39}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_map = model.config.id2label\n",
    "results = []\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = label_map[pred]\n",
    "    if label != zero_label:\n",
    "        # remove B- or I- in the label\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "        while idx < len(predictions) and label_map[predictions[idx]] == f\"I-{label}\":\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "        \n",
    "        word = example[start:end]\n",
    "        results.append({\"entity_group\": label, \"word\": word, \"start\": start, \"end\": end})\n",
    "    idx += 1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside question answering pipeline\n",
    "#### The question-answering pipeline finds the answer to questions in agiven context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/qiyaoxue/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9557097554206848,\n",
       " 'start': 82,\n",
       " 'end': 109,\n",
       " 'answer': 'Jax, Pytorch and TensorFlow'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answer = pipeline(\"question-answering\")\n",
    "context = \"HuggingFace Transformers is backed by the three most popular learning libraries - Jax, Pytorch and TensorFlow - with between them. It's straightforward to train your models with one before loading them forinference with the other\"\n",
    "question =\"Which deep learning libraries back HuggingFace Transformers\"\n",
    "question_answer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenization and model\n",
    "#### The model have two outputs represent the start label and the end label of the answer to the question in all inputs\n",
    "<div><img src=\"image/pipeline4.png\" width=800></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-5.2959, -5.6157, -3.8885, -5.3484, -7.1676, -6.8139, -4.4925, -6.9972,\n",
      "         -6.4198, -7.3189, -5.1918, -5.6344, -0.8541, -5.1717, -4.5820, -5.7030,\n",
      "         -2.0335, -3.0432, -1.3448, -2.5769,  2.0164,  3.1982, -1.6996, -2.3744,\n",
      "         -1.8704, -2.4196,  0.7976,  9.3653, -2.6236,  5.8951, -2.6435, -3.6129,\n",
      "         -2.6053, -2.7659,  0.3541, -2.3827, -2.3757, -1.8303, -3.5220, -2.1199,\n",
      "         -2.3484, -3.6579, -3.9880, -2.0975, -6.1880, -7.0002, -4.0363, -6.3267,\n",
      "         -3.7176, -4.3637, -3.4225, -7.2126, -5.4035, -6.3427, -4.4348, -6.9884,\n",
      "         -5.9979, -7.8049, -7.3074, -8.1919, -6.9520, -7.7807, -5.6345]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[-2.8224, -7.4711, -6.3334, -5.0852, -5.1913, -8.1811, -8.0520, -6.4976,\n",
      "         -7.3729, -4.2701, -4.4396, -5.2550, -5.9191, -4.8538, -5.5134, -2.0127,\n",
      "         -1.1316, -5.3485, -4.3896, -6.0071, -5.1289,  1.7767, -5.2148, -2.1351,\n",
      "         -2.1775,  1.5612, -2.5911,  2.3636, -0.0332, -2.0562, -2.8141, -2.1260,\n",
      "          3.0627, -1.4925, -0.0568,  0.6057, -0.3598,  9.1709,  3.4657, -1.0581,\n",
      "         -2.1748,  3.1914,  1.6916, -6.7120, -7.5602, -7.9499, -5.6982, -8.3308,\n",
      "         -5.1490, -6.1176, -2.4890, -7.1836, -3.0868, -6.3731, -5.8592, -5.1120,\n",
      "         -7.2861, -7.5542, -3.6584, -5.7124, -5.9768, -1.8662, -5.2550]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
      "torch.Size([1, 63]) torch.Size([1, 63]) torch.Size([1, 63])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_ckpt = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(inputs[\"input_ids\"].shape, start_logits.shape , end_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before applying the Softmax, we mask the logits outside of teh context, the sentence id input to the tokenizer and be checked using the ***sequence_ids()*** with the tokenized result\n",
    "<img src=\"image/pipeline5.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
      "tensor([4.1497e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        3.5241e-05, 4.6986e-07, 8.4738e-07, 2.7620e-07, 1.0836e-05, 3.9477e-06,\n",
      "        2.1576e-05, 6.2934e-06, 6.2191e-04, 2.0275e-03, 1.5132e-05, 7.7056e-06,\n",
      "        1.2756e-05, 7.3650e-06, 1.8382e-04, 9.6675e-01, 6.0064e-06, 3.0075e-02,\n",
      "        5.8876e-06, 2.2334e-06, 6.1173e-06, 5.2094e-06, 1.1797e-04, 7.6419e-06,\n",
      "        7.6957e-06, 1.3277e-05, 2.4459e-06, 9.9395e-06, 7.9090e-06, 2.1350e-06,\n",
      "        1.5347e-06, 1.0164e-05, 1.7005e-07, 7.5482e-08, 1.4624e-06, 1.4803e-07,\n",
      "        2.0112e-06, 1.0541e-06, 2.7017e-06, 6.1038e-08, 3.7264e-07, 1.4568e-07,\n",
      "        9.8175e-07, 7.6381e-08, 2.0566e-07, 3.3757e-08, 5.5521e-08, 2.2924e-08,\n",
      "        7.9214e-08, 3.4584e-08, 0.0000e+00], grad_fn=<SqueezeBackward0>) \n",
      " tensor([6.1147e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        2.7638e-07, 8.0194e-07, 4.1466e-07, 1.3742e-05, 3.3166e-05, 4.8900e-07,\n",
      "        1.2757e-06, 2.5311e-07, 6.0910e-07, 6.0777e-04, 5.5897e-07, 1.2159e-05,\n",
      "        1.1653e-05, 4.8998e-04, 7.7064e-06, 1.0930e-03, 9.9480e-05, 1.3157e-05,\n",
      "        6.1658e-06, 1.2269e-05, 2.1992e-03, 2.3118e-05, 9.7156e-05, 1.8844e-04,\n",
      "        7.1756e-05, 9.8858e-01, 3.2907e-03, 3.5694e-05, 1.1685e-05, 2.5013e-03,\n",
      "        5.5818e-04, 1.2507e-07, 5.3553e-08, 3.6271e-08, 3.4471e-07, 2.4781e-08,\n",
      "        5.9696e-07, 2.2663e-07, 8.5348e-06, 7.8042e-08, 4.6943e-06, 1.7552e-07,\n",
      "        2.9343e-07, 6.1950e-07, 7.0438e-08, 5.3877e-08, 2.6503e-06, 3.3982e-07,\n",
      "        2.6088e-07, 1.5910e-05, 0.0000e+00], grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "print(sequence_ids)\n",
    "mask = [ i != 1 for i in sequence_ids]\n",
    "# unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask).unsqueeze(dim=0)\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "start_probabilites = torch.nn.functional.softmax(start_logits, dim=-1).squeeze()\n",
    "end_probabilites = torch.nn.functional.softmax(end_logits, dim=-1).squeeze()\n",
    "print(start_probabilites, \"\\n\", end_probabilites)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An answer isa pair of start and end positions\n",
    "<div><img src=\"image/pipeline6.png\" width=800></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5375e-12, 0.0000e+00, 0.0000e+00,  ..., 1.0826e-13, 6.6022e-12,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0666e-14, 1.2603e-12,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.5022e-13,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<TriuBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores = start_probabilites.unsqueeze(dim=-1) * end_probabilites.unsqueeze(dim=0)\n",
    "scores = scores.triu()\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After finding the possible answer with the best score, we use the offset mappings to find the corresponding answer in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 63\n",
      "answer: 'Jax, Pytorch and TensorFlow', score: 0.9557098746299744\n"
     ]
    }
   ],
   "source": [
    "score = scores.max().item()\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "\n",
    "input_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = input_with_offsets[\"offset_mapping\"]\n",
    "print(len(input_with_offsets[\"input_ids\"]), len(offsets))\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "\n",
    "print(f\"answer: '{answer}', score: {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
